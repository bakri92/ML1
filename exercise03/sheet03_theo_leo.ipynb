{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise sheet 03\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Lagrange Multipliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the function $J(\\theta) = \\sum_{k}^{n} ||\\theta - x_k||^2$ with parameter $\\theta \\in \\mathbb{R}^d$:\n",
    "\n",
    "We define $\\overline{x} = \\frac{1}{n} \\sum_k^n x_k$.\n",
    "\n",
    "(a) Find $\\theta$ that minimizes $J(\\theta)$ under the constraint $\\theta^T b = 0$ with $ b \\in \\mathbb{R}^d$:\n",
    "\n",
    "$L(\\theta, \\lambda) = \\sum_{k}^{n} ||\\theta - x_k||^2 + \\lambda \\cdot (\\theta^T b)$ and $\\nabla L(\\theta, \\lambda) = \\theta^T b + \\sum_{k}^{n} \\sum_i^d 2(\\theta - x_k)_i + \\lambda b$>\n",
    "\n",
    "Setting $\\nabla L(\\theta, \\lambda) = 0$ we obtain $\\theta = \\overline{x} + \\frac{\\lambda b}{2 \\cdot n}$ \n",
    "\n",
    "Geometrical interpretation: The empirical mean with a shift directly proportional to $b$ and inverse proportional to the number of samples $n$. Because of $\\theta^T b = 0$, $\\theta$ and $b$ are perpendicular.\n",
    "\n",
    "(b) Find $\\theta$ that minimizes $J(\\theta)$ under the constraint $||\\theta - c||^2 = 1$ with $ c \\in \\mathbb{R}^d$:\n",
    "\n",
    "$L(\\theta, \\lambda) = \\sum_{k}^{n} ||\\theta - x_k||^2 + \\lambda \\cdot (||\\theta - c||^2 -1)$ and $\\nabla L(\\theta, \\lambda) = \\sum_k^n \\sum_i^d 2 (\\theta - x_k)_i +2 \\lambda \\sum_i^d (\\theta -c)_i -1 + \\sum_i^d (\\theta - c)_i^2$\n",
    "\n",
    "Setting $\\nabla L(\\theta, \\lambda) = 0$ we obtain $\\theta = \\frac{n}{n-1} \\overline{x} - \\frac{c}{n-1}$ with $||\\theta - c||^2 = 1$\n",
    "\n",
    "Geometrical interpretation: For large samples, $\\theta$ approaches the empirical mean shifted by $\\frac{c}{n-1}$. If also $c << n$, $\\theta$ approaches the empirical mean. The distance between $c$ and $\\theta$ is hold constantly at $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Bounds on Eigenvalues:\n",
    "Dataset $x_n \\in \\mathbb{R}^d$, empirical mean $m = \\frac{1}{n} \\sum_k^n x_k$ and Scatter matrix $S = \\sum_k^n (x_k -m)(x_k - m)^T$\n",
    "\n",
    "With $\\lambda_1$ beeing the largest eigenvalue of $S$ and $S_{ii}$ the diagonal elements of $S$\n",
    "\n",
    "(a) Upper bounds of the eigenvalue $\\lambda_1$:\n",
    "\n",
    "With the trace beeing the sum over all eigenvalues the term $\\sum_k^n S_{ii}$ is an upper bound for $\\lambda_1$,\n",
    "because of the postive semidefiniteness of the scatter Matrix $S$.\n",
    "\n",
    "proof: consideration of a random column vector $a$\n",
    "\n",
    "$a^T S a = a^T (\\sum_k^n (x_k - m)(x_k - m)^T) a = \\sum_k^n (a^T (x_k - m))^2$\n",
    "\n",
    "Since $a^T, x_k$ and $m$ are real valued, this is greater or equal to zero. THerefore all eigenvalues ar egreater or equal to zero.\n",
    "\n",
    "(b) Dataconditions for which the upper bound is tight:\n",
    "\n",
    "that would be that all the datavariation is along PCA1 and nothing around the Rest, eg. all other eigenvectors are zero.\n",
    "\n",
    "(c) Lower bounds of the eigenvalue $\\lambda_1$:\n",
    "\n",
    "all positiv => minimum\n",
    "\n",
    "(d) Dataconditions for which the lower bound is tight:\n",
    "\n",
    "If there is no PCA(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
